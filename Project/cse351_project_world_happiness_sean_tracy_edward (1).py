# -*- coding: utf-8 -*-
"""cse351_project_world_happiness_sean_tracy_edward.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J-zHJSh4rxPzORjSimNzPBvk-xA_MNUc

# World Happiness

The World Happiness Report provides an annual analysis of what factors contribute to people's well-being and happiness. They examine many features that range from country economics to self-reported polls. For this project, we would like to examine factors that contribute to happiness, much like the World Happiness Report.

## Obtaining the Data

We obtained the rankings found in global polls about how people ranked their happiness and various aspects of their lives from 2015 to 2019. The data points are separated by country and region. We start by concatenating all the data together for easier cleaning and managing. We would also like to add another column to label the year of each data point.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from scipy import stats

# Configure Pandas display settings.
pd.options.display.max_rows = 15

# Import the data sets.
happy_list = []
years = [2015, 2016, 2017, 2018, 2019]
for year in years:
  happy_list.append(pd.read_csv(str(year) + '.csv'))

# Add a column to each dataset for the year.
for happy, year in zip(happy_list, years):
  happy['Year'] = year

happy15, happy16, happy17, happy18, happy19 = happy_list

# Concatenate all the data together.
happy = pd.concat(happy_list)

happy

"""## Cleaning the Data

Before we can start working with the data, we must ensure that it is properly combined, cleaned, and free of errors.

### Name Unification

The concatenation did not properly occur because it generated large columns with no values. With a quick review, it is apparent that the naming conventions of the columns have changed. To fix this, we must normalize all columns between the years.

Our first step is to ensure that the columns for each year correspond to each other. For instance, the data for 2017 are not separated by whitespace. Instead, it is separated by periods. We simply rename each column for 2017.
"""

happy17.rename(columns={
    'Happiness.Score': 'Happiness Score',
    'Happiness.Rank': 'Happiness Rank',
    'Whisker.low': 'Lower Confidence Interval',
    'Whisker.high': 'Upper Confidence Interval',
    'Economy..GDP.per.Capita.': 'Economy (GDP per Capita)',
    'Health..Life.Expectancy.': 'Health (Life Expectancy)',
    'Trust..Government.Corruption.': 'Trust (Government Corruption)',
    'Dystopia.Residual': 'Dystopia Residual'
}, inplace=True)

happy17

"""Furthermore, the data for 2018 and 2019 contain the same information but follow a different naming scheme than the previous year. We also need to rename each of these columns."""

for happy in [happy18, happy19]:
    happy.rename(columns={
        'Overall rank': 'Happiness Rank',
        'Country or region': 'Country',
        'Score': 'Happiness Score',
        'GDP per capita': 'Economy (GDP per Capita)',
        'Social support': 'Family',
        'Healthy life expectancy': 'Health (Life Expectancy)',
        'Freedom to make life choices': 'Freedom',
        'Perceptions of corruption': 'Trust (Government Corruption)'
    }, inplace=True)

happy18

"""Next, between the data sets, some contain a column for standard error in the happiness score. Others, instead, have the upper and lower confidence intervals. We can combine the two columns by aggregating the upper and lower confidence levels. We create a new column for the standard error and calculate it as the difference between the upper confidence interval and the happiness score.

"""

for happy in [happy16, happy17]:
    happy['Standard Error'] = happy['Upper Confidence Interval'] - \
                              happy['Happiness Score']
    happy.drop(columns=['Upper Confidence Interval',
                        'Lower Confidence Interval'], inplace=True)

happy16

"""### Missing Values

After renaming the columns, we can check for any missing values in the combined data set. We start by recombining the edited data columns into one data frame.
"""

happy = pd.concat(happy_list)
happy.reset_index(drop=True, inplace=True)
happy

"""We now summarize any missing values that are in the data frame."""

happy.isnull().sum()

happy[happy['Region'].isnull() |
      happy['Standard Error'].isnull() |
      happy['Trust (Government Corruption)'].isnull() |
      happy['Dystopia Residual'].isnull()]

"""Upon closer inspection, there are four columns with missing values: region, standard error, trust, and dystopia residual.

#### Missing Regions

The missing values for the region seem to be missing not at random (MNAR) because there are some years where the data was not included. Each country corresponds to a larger area from another year's data set. We can simply map the missing values because there are no missing values from the country field.
"""

# Create a dictionary mapping country to region from 2015 and 2016.
regions = dict(zip(happy15['Country'], happy15['Region']))
regions.update(dict(zip(happy16['Country'], happy16['Region'])))

# Apply mapping to each missing region.
happy['Region'].fillna(happy['Country'].map(regions), inplace=True)
happy

happy[happy['Region'].isnull()]

"""When reexamining the count of missing region values, eight remain empty. These can be attributed to a break in naming conventions. We can simply unify the country names or fill each region in manually to complete the missing values."""

# Unify country names.
happy.at[347, 'Country'] = 'Taiwan'
happy.at[385, 'Country'] = 'Hong Kong'
happy.at[507, 'Country'] = 'Trinidad and Tobago'
happy.at[527, 'Country'] = 'North Cyprus'
happy.at[664, 'Country'] = 'Trinidad and Tobago'
happy.at[689, 'Country'] = 'North Cyprus'
happy.at[709, 'Country'] = 'Macedonia'

# Fill region by mapping.
happy['Region'].fillna(happy['Country'].map(regions), inplace=True)

# Set the region for countries not in the mapping.
happy.at[745, 'Region'] = 'Sub-Saharan Africa'

happy

"""#### Missing Standard Error

There are also missing values for the standard error. The types of missing values is also MNAR because some of the more recent years omitted a range of errors for the happiness scores. We can simply fill these in with zeros to indicate no error range. It will allow us to keep the standard error for the older years.
"""

happy['Standard Error'].fillna(0.0, inplace=True)
happy

"""#### Missing Trust Value

It should be noted that there is one value missing from the trust column. Seemingly, the value is missing completely at random (MCAR) because there is no indication of why it is not there. We would like to perform a mean imputation to fill in the value. The missing trust rank would be replaced with the average of the trust ranks of the United Arab Emirates from the other years.
"""

happy_uae = happy[happy['Country'] == 'United Arab Emirates']
mean_trust_uae = np.mean(happy_uae['Trust (Government Corruption)'])

happy.loc[489, 'Trust (Government Corruption)'] = mean_trust_uae
mean_trust_uae

"""#### Missing Dystopia Residuals

Lastly, many values are missing from the dystopia residual field. As with many of the missing parameters before, it is MNAR because recent years elected to omit the score. In this case, we would like to just drop the variable from our data set because it does not serve a great purpose.
"""

happy.drop(columns=['Dystopia Residual'], inplace=True)

happy

"""### Outlying Values

After filling in the missing values, we must find outlying values in the data set. Outlying values should be marked and imputed to generate more accurate models. We should check each score: economy, family, health, freedom, trust, and generosity.

Our method for detecting outliers is to first visualize the frequency distribution of the scores. We then check if the distribution shape is similar to a "normal" curve. If so, we can confidently say that any point that falls outside of three standard deviations from the mean is an outlier.

To start, we define some variables and functions to help visualize these frequencies. Each visualization will be a histogram where the scores are binned in 0.0625-intervals.
"""

# Colors
ivory = '#F2F5EA'
persian_green = '#00A896'
slate_gray = '#748189'
blush = '#E75A7C'
gunmetal = '#2C363F'

# Plot Functions
def set_color(fig, ax):
    fig.patch.set_facecolor(ivory)
    ax.yaxis.label.set_color(gunmetal)
    ax.xaxis.label.set_color(gunmetal)
    ax.title.set_color(gunmetal)
    for spine in ax.spines.values():
        spine.set_edgecolor(gunmetal)


def show_frequency(score, x_min=0.0, x_max=3.0, group=0.0625,
                   color=persian_green):
    fig, ax = plt.subplots(figsize=[7.5, 5.0], dpi=100)

    set_color(fig, ax)

    ax.set(xlim=(x_min, x_max), xticks=np.arange(x_min, x_max + 0.25, 0.25),
           xlabel=f'{score} Score', ylabel='Frequency',
           title=f'Frequency of {score} Scores', facecolor=ivory)

    ax.hist(happy[score], bins=np.arange(x_min, x_max, group),
            color=color, edgecolor=ivory)

    return fig, ax

"""#### Outlying Economy Scores"""

show_frequency('Economy (GDP per Capita)')

"""We see that the frequency distribution of economic scores is relatively normally-distributed, with its bell shape. Thus, we can detect if there are any outliers by computing the Z-score for each point and checking if they are greater than 3 or less than -3."""

happy[np.abs(stats.zscore(happy['Economy (GDP per Capita)'])) > 3.0]

"""As we can see, there are no outlying values for the economic scores.

#### Outlying Family Scores
"""

show_frequency('Family', x_max=2.0)

happy[np.abs(stats.zscore(happy['Family'])) > 3.0]

"""Some outlying family scores fall out by three standard deviations from the mean. We simply substitute these values with the mean family score to address the issue. It should be noted that these values are correlated with the country. It indicates that although they are outlying values, they may not have occurred because of data entry errors."""

happy.loc[np.abs(stats.zscore(happy['Family'])) > 3.0, 'Family'] = np.nan
happy.fillna(np.nanmean(happy['Family']), inplace=True)
happy

"""#### Outlying Health Scores"""

show_frequency('Health (Life Expectancy)', x_max=1.5)

happy[np.abs(stats.zscore(happy['Health (Life Expectancy)'])) > 3.0]

"""Similar to the economic scores, the distribution of health scores visually fits a normal distribution. Furthermore, there are no outliers in the health scores.

#### Outlying Freedom Scores
"""

show_frequency('Freedom', x_max=1.0, group=0.0625)

happy[np.abs(stats.zscore(happy['Freedom'])) > 3.0]

"""As with the previous economic and health scores, there are no outlying values in the freedom scores either.

#### Outlying Trust Scores

"""

show_frequency('Trust (Government Corruption)', x_max=1.0, group=0.0625)

happy[np.abs(stats.zscore(happy['Trust (Government Corruption)'])) > 3.0]

"""The trust scores contain the most outliers within any of the categories. As with previous scores, the outlying points are correlated with specific countries. Thus, it should also be noted that the outlying trust scores are not due to data entry errors. Nonetheless, we would like to substitute these values with the mean."""

happy.loc[np.abs(stats.zscore(happy['Trust (Government Corruption)'])) > 3.0,
          'Trust (Government Corruption)'] = np.nan

happy.fillna(np.nanmean(happy['Trust (Government Corruption)']), inplace=True)
happy

"""#### Outlying Generosity Scores"""

show_frequency('Generosity', x_max=1.0, group=0.0625)

happy[np.abs(stats.zscore(happy['Generosity'])) > 3.0]

"""Lastly, there are several outliers for the generosity score. As with every other category, we provide the same treatment. We replace the outliers with the mean."""

happy.loc[np.abs(stats.zscore(happy['Generosity'])) > 3.0, 'Generosity'] = \
    np.nan

happy.fillna(np.nanmean(happy['Generosity']), inplace=True)
happy

"""## Exploratory Data Analysis

With the data cleaned, we would like to perform an overall analysis to capture trends and correlations in the data. Most notably, we would like to see how the happiness scores change over time and which features heavily contribute to a higher happiness score.

### Happiness Score Central Tendencies

First, we would like to take the central tendencies of the happiness scores for each year. We do so by grouping the data by year and aggregating the happiness scores into the mean and median. Note that we do not consider the mode because the happiness scores are on a continuous interval. Thus, it will only determine which values are repeated because of a coincidence.
"""

central_happy = happy.groupby('Year').agg(
    mean=('Happiness Score', np.mean),
    median=('Happiness Score', np.median)
).rename(columns={
    'mean': 'Mean',
    'median': 'Median'
})

central_happy

"""Now, we plot the central tendencies on a line graph to visualize the changes over the years."""

cnt_hap_fig, cnt_hap_ax = plt.subplots(figsize=[7.5, 5], dpi=100)
set_color(cnt_hap_fig, cnt_hap_ax)

cnt_hap_ax.set(xlim=(2014, 2020), xticks=np.arange(2015, 2020, 1),
               xlabel='Year', ylim=(5.2, 5.45),
               yticks=np.arange(5.2, 5.50, 0.05), ylabel='Happiness Score',
               title='Happiness Score over Years', facecolor=ivory)

cnt_hap_ax.plot(central_happy.index, central_happy['Mean'], c=blush)

cnt_hap_ax.plot(central_happy.index, central_happy['Median'], c=persian_green)

cnt_hap_ax.legend(['Mean', 'Median'], facecolor=ivory, edgecolor=gunmetal)

"""From the line graph, both the means and medians generally increase. It indicates that the happiness score tends to be raised throughout time. However, it is interesting that both central tendencies dipped in 2017. It can suggest that there may have been a global event that affected the well-being and happiness of individuals during that time.

### Happiness Ranks

Next, we would like to check which countries improved or maintained their happiness rankings. We need to have a clear definition of improving a rank and what it means to have a steady score.
For a country to have improved its ranking throughout five years, its rank must have increased every year. For a country to have a steady happiness level, its score must not change or incur the least amount of change throughout the years.
"""

# Calculates total change in a series.
def get_total_change(series):
    total_change = 0
    last = 0
    for index, value in series.items():
        if last >= 0:
            total_change += np.abs(value - last)

        last = value
    return total_change

# Groups data by country and then calculates the total change in happiness rank
# for each country.
steady_happy = happy.groupby('Country').agg(
    total_change=('Happiness Rank', get_total_change),
    minimum_rank=('Happiness Rank', np.min),
    maximum_rank=('Happiness Rank', np.max)
).rename(columns={
    'total_change': 'Total Happiness Rank Change',
    'minimum_rank': 'Minimum Happiness Rank',
    'maximum_rank': 'Maximum Happiness Rank'
}).reset_index().sort_values('Total Happiness Rank Change')

steady_happy

def is_decreasing(series):
    last = float('inf')
    for index, value in series.items():
        if value > last:
            return False
        last = value
    return True

increase_happy = happy.groupby('Country').filter(
    lambda df : is_decreasing(df['Happiness Rank'])
).groupby('Country').agg(
    total_change=('Happiness Rank', get_total_change)
).reset_index(
).rename(columns={'total_change': 'Total Change'}
).sort_values('Total Change', ascending=False)

increase_happy

"""### Visualize the relationship between happiness score and other features such as GDP, social support, freedom, etc."""

#create a heatmap so see the relationship between the happiness score and other features

import seaborn as sns

plt.figure(figsize=(10, 5))
sns.heatmap(happy.corr(), cmap="coolwarm", annot = True)
plt.title('Correlation between features of World Happiness')
plt.show()

"""### Find out what features contribute to happiness. If you are the president of a country, what would you do to make citizens happier?

From the heatmap, we can see that Happiness Score's correlation with other features from most correlated to least correlated is Economy, Health, Family, Freedom , Trust, and then Generosity. With a correlation of greater than 0.7, Happiness Score in a country and more tied in with their GDP and their life expectancy. 


Therefore if we were the president of a country, we would first increase the GDP per capita of the country as there is a strong correlation between the GDP and Happiness score. To increase the GDP, we would improve the quality of education and increase job skills within the country. This will also increase the life expectancy of the country as it is highly correlated with GDP.

# Modeling and Question Answering

### Model 1: Linear Regression

Linear Regression is a model to determine the relationship between numerical features. It is very easy to understand and makes a good baseline model to understand the dataset.
"""

# seperate into training and testing sets
testing = happy.loc[happy['Year'] == 2019]
training =happy.loc[happy['Year'].isin([2015, 2016, 2017, 2018])]

#create training data and testing data
#training data
train_lr_x = training[['Economy (GDP per Capita)', 'Freedom']]
train_lr_y= training['Happiness Score']
#testing data
test_lr_y = testing['Happiness Score']
test_lr_x = testing[['Economy (GDP per Capita)', 'Freedom']]

"""For our training data, we only use the 'Economy (GDP per Capita)' and 'Freedom' feature as those two features are not highly correlated with each other and they have high correlation with 'Happiness Score'."""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

happinessLR = LinearRegression()
happinessLR.fit(train_lr_x, train_lr_y)

y_predict = happinessLR.predict(test_lr_x)
y_predict

import matplotlib.pyplot as plt
plt.figure(figsize=(10,5))
plt.scatter(test_lr_y, y_predict)
xpoints = np.array([2.5, 3, 4, 5, 6, 7, 8])
ypoints = np.array([2.5, 3, 4, 5, 6, 7, 8])
plt.plot(xpoints, ypoints, linestyle = 'dotted')
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.title('Actual vs. Predicted')

np.column_stack((y_predict, test_lr_y))

rms = mean_squared_error(test_lr_y, y_predict, squared=False) 
print(rms)

"""The root mean squared shows us that the standard deviation of the predicated values compared to the actual values is only 0.6."""

#compare the 
Predictions = pd.DataFrame({'Prediction Happiness': y_predict, 'Actual Happiness': test_lr_y})
Predictions.insert(0, 'Actual Rankings', range(1, 1+len(Predictions)))
Predictions = Predictions.sort_values(by='Prediction Happiness', ascending=False)
Predictions.insert(0, 'Predicted Rankings', range(1, 1+len(Predictions)))
print(Predictions.to_markdown())

"""### Model 2: Ridge Regression
Other options? Least Absolute Deviation Regression

"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt

# seperate into training and testing sets
testing2 = happy.loc[happy['Year'] == 2019]
training2 = happy.loc[happy['Year'].isin([2015, 2016, 2017, 2018])]

target_column2 = ['Happiness Score']
not_used2 =  ['Country', 'Region', 'Happiness Rank', 'Year', 'Standard Error']
excluded2 = target_column2 + not_used2
excluded2

predictors2 = list(set(list(training2.columns))-set(excluded2))

X2 = training2[predictors2].values
Y2 = training2[target_column2].values

X2_train, X2_test, Y2_train, Y2_test = train_test_split(X2, Y2, test_size=0.30, random_state=40)

print(X2_train.shape); print(X2_test.shape)

from sklearn.linear_model import Ridge

# define model
ridgeReg = Ridge(alpha=1.0)


ridgeReg.fit(X2_train, Y2_train) 
pred_train_ridgeReg = ridgeReg.predict(testing2.drop(excluded2, axis = 1).values)

rms2 = mean_squared_error(testing2['Happiness Score'], pred_train_ridgeReg, squared=False) 
print(rms2)

predictedRankings2 = testing2.copy()

predictedRankings2['Happiness Score'] = pred_train_ridgeReg

testing2

predictedRankings2

predictedRankings2['Happiness Rank'] = predictedRankings2['Happiness Score'].rank(method='max', ascending=False)

predictedRankings2

"""## Model 3: Robust Regresssion (RANdom SAmple Consensus (RANSAC))"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from math import sqrt

# seperate into training and testing sets
testing3 = happy.loc[happy['Year'] == 2019]
training3 = happy.loc[happy['Year'].isin([2015, 2016, 2017, 2018])]

target_column3 = ['Happiness Score']
not_used3 =  ['Country', 'Region', 'Happiness Rank', 'Year', 'Standard Error']
excluded3 = target_column3 + not_used3
excluded3

predictors3 = list(set(list(training3.columns))-set(excluded3))

X3 = training3[predictors3].values
Y3 = training3[target_column3].values

X3_train, X3_test, Y3_train, Y3_test = train_test_split(X3, Y3, test_size=0.30, random_state=40)

print(X3_train.shape); print(X3_test.shape)

from sklearn.linear_model import RANSACRegressor

ransac = RANSACRegressor(LinearRegression(),
		max_trials=100, 		# Number of Iterations
		min_samples=20, 		# Minimum size of the sample
		loss='absolute_error', 	# Metrics for loss #absolute_loss was before
		residual_threshold=10 	# Threshold
		)

# Train model
ransac.fit(X3_train, Y3_train)

pred_train_ransac = ransac.predict(testing3.drop(excluded3, axis = 1).values)

rms4 = mean_squared_error(testing3['Happiness Score'], pred_train_ransac, squared=False) 
print(rms4)

"""# Formula to calculate Happiness score"""

ridgeReg.coef_

ridgeReg.intercept_

"""Our formula for Happiness Score based on Ridge Regression is:

$$
  Happiness Score = (0.62788957 * Economy) + (0.62607593 * Family) + (1.28876482 * Health) + (1.04780362*Freedom) + (1.37226637 * Trust) + (1.2066021 * Generosity) + 2.21216454
$$
"""

